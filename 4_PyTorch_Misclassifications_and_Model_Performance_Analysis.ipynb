{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/ModernCompVision/blob/01_OpenCV/4_PyTorch_Misclassifications_and_Model_Performance_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpaTpZ-2GWOW"
      },
      "source": [
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/logo_MCV_W.png)\n",
        "\n",
        "# **PyTorch Model Performance Analysis**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In this lesson, we learn use the MNIST model we trained in the previously lesson and analyze it's performance, we do:\n",
        "1. Setup Our PyTorch Model and Data\n",
        "2. Load the previously trained model\n",
        "3. View the images we misclassified\n",
        "4. Create a Confusion Matrix\n",
        "5. Create Classification Report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChwAN8-RG6dG"
      },
      "source": [
        "# **1. Setup our PyTorch Imports, Model and Load the MNIST Dataset**\n",
        "\n",
        "We only need to load the Test dataset since we're analyzing performance on that data segment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocGThQeN9SP9",
        "outputId": "9828cdb0-0716-4307-ba62-bc287d1073f1"
      },
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "# We use torchvision to get our dataset and useful image transformations\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Import PyTorch's optimization libary and nn\n",
        "# nn is used as the basic building block for our Network graphs\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Are we using our GPU?\n",
        "print(\"GPU available: {}\".format(torch.cuda.is_available()))\n",
        "\n",
        "# Set device to cuda\n",
        "device = 'cuda'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGo1tg0AHRNE"
      },
      "source": [
        "#### **Our Image plotting function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEBJZR9jC1W7"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define our imshow function\n",
        "def imgshow(title, image = None, size = 6):\n",
        "      w, h = image.shape[0], image.shape[1]\n",
        "      aspect_ratio = w/h\n",
        "      plt.figure(figsize=(size * aspect_ratio,size))\n",
        "      plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "      plt.title(title)\n",
        "      plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the function  imgshow\n",
        "\n",
        "-----------------------------\n",
        "# Import necessary libraries:\n",
        "\n",
        "#import cv2:\n",
        "Imports the OpenCV library (cv2) for image processing.\n",
        "\n",
        "##import numpy as np:\n",
        "Imports the NumPy library (np) for numerical operations, particularly for working with arrays.\n",
        "\n",
        "###from matplotlib import pyplot as plt:\n",
        "Imports the pyplot module from Matplotlib (plt) for creating visualizations, especially plots and images.\n",
        "\n",
        "------------------------------\n",
        "#Define the imgshow function\n",
        "\n",
        "    def imgshow(title, image=None, size=6):\n",
        "    \n",
        "This line defines a function named\n",
        "\n",
        "##imgshow that takes three arguments:\n",
        "\n",
        "###title:\n",
        "A string representing the title of the image display.\n",
        "\n",
        "###image:\n",
        "The image data itself (presumably a NumPy array). It defaults to None if not provided.\n",
        "\n",
        "###size:\n",
        "A numerical value controlling the display size of the image (defaulting to 6).\n",
        "\n",
        "-----------------------------------\n",
        "#Calculate aspect ratio:\n",
        "\n",
        "##w, h = image.shape[0], image.shape[1]:\n",
        "These lines extract the width (w) and height (h) of the input image from its shape attribute.\n",
        "\n",
        "##aspect_ratio = w/h:\n",
        "This line calculates the aspect ratio of the image by dividing the width by the height.\n",
        "\n",
        "---------------------------------\n",
        "#Create a Matplotlib figure:\n",
        "\n",
        "##plt.figure(figsize=(size * aspect_ratio, size)):\n",
        "This line creates a new Matplotlib figure with a specific size.\n",
        "\n",
        "##plt.figure():\n",
        "This function creates a new figure window in which the image will be displayed.\n",
        "\n",
        "A figure is like a container that holds one or more plots or images.\n",
        "\n",
        "##figsize=(size * aspect_ratio, size):\n",
        "This argument specifies the size of the figure window.\n",
        "\n",
        "figsize is a keyword argument that takes a tuple representing the width and height of the figure in inches.\n",
        "\n",
        "size * aspect_ratio is the calculated width of the figure.\n",
        "\n",
        "size is the height of the figure.\n",
        "\n",
        "Let's consider an example:\n",
        "\n",
        "Suppose you want to display an image with an aspect ratio of 1.5 (width is 1.5 times the height), and you want the height of the displayed image to be 6 inches. Then, this line would become:\n",
        "\n",
        "    plt.figure(figsize=(6 * 1.5, 6))\n",
        "\n",
        "This means the figure window would be created with a width of 9 inches (6 * 1.5) and a height of 6 inches.\n",
        "\n",
        "This ensures that the image is displayed with the correct aspect ratio, preventing distortion.\n",
        "\n",
        "##In summary:\n",
        "\n",
        "The line plt.figure(figsize=(size * aspect_ratio, size)) is crucial for creating a figure window with the appropriate dimensions to display the image without distortion.\n",
        "\n",
        "It calculates the width based on the aspect ratio and desired height, providing a visually accurate representation of the image.\n",
        "\n",
        "---------------------\n",
        "#Display the image:\n",
        "\n",
        "##plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)):\n",
        "This line displays the image using Matplotlib's imshow function.\n",
        "\n",
        "Note that OpenCV often stores images in BGR (Blue, Green, Red) format, so this line converts the image to RGB (Red, Green, Blue) format before displaying it using Matplotlib.\n",
        "\n",
        "----------------------------\n",
        "#Set the title:\n",
        "\n",
        "##plt.title(title):\n",
        "\n",
        "This line sets the title of the image display using the provided title argument.\n",
        "\n",
        "--------------------------\n",
        "#Show the plot:\n",
        "\n",
        "##plt.show():\n",
        "\n",
        "This line finally displays the Matplotlib figure containing the image and its title.\n",
        "\n",
        "In essence, the imgshow function provides a convenient way to display images within your Colab notebook using OpenCV and Matplotlib, ensuring the image is presented with the correct aspect ratio and a title.\n",
        "\n",
        "This helps in visualizing images during data exploration or model analysis tasks."
      ],
      "metadata": {
        "id": "Z8v-mzEPQ3UH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkQReXp-HdFo"
      },
      "source": [
        "### **Loading our MNIST Test Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeiahvMrChw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d1a476-301f-4fe2-a475-abb10fe13ec4"
      },
      "source": [
        "# Transform to a PyTorch tensors and the normalize our valeus between -1 and +1\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, ), (0.5, )) ])\n",
        "\n",
        "# Load our Test Data and specify what transform to use when loading\n",
        "testset = torchvision.datasets.MNIST('mnist',\n",
        "                                     train = False,\n",
        "                                     download = True,\n",
        "                                     transform = transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                          batch_size = 128,\n",
        "                                          shuffle = False,\n",
        "                                          num_workers = 0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.59MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 133kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.09MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.30MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain multiple image transformations together.\n",
        "This function from torchvision.transforms allows you to chain multiple image transformations together.\n",
        "\n",
        "------------------------------\n",
        "---------------------------------------\n",
        "##transforms.ToTensor():\n",
        "\n",
        "This transformation converts the PIL Image (a common image format) representing each MNIST digit into a PyTorch tensor.\n",
        "\n",
        "This is necessary for PyTorch to work with the data.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "#1. transforms.Normalize((0.5, ), (0.5, )):\n",
        "This transformation normalizes the pixel values of the image.\n",
        "\n",
        "It subtracts the mean (0.5) from each pixel and then divides by the standard deviation (0.5).\n",
        "\n",
        "This helps improve the performance of the model by ensuring the pixel values are within a specific range (usually between -1 and 1).\n",
        "\n",
        "The arguments (0.5, ) and (0.5, ) represent the mean and standard deviation for each color channel (since MNIST images are grayscale, there's only one channel).\n",
        "\n",
        "--------------------\n",
        "#2. Loading the MNIST Dataset:\n",
        "\n",
        "\n",
        "## Load our Test Data and specify what transform to use when loading\n",
        "    testset = torchvision.datasets.MNIST\n",
        "        ('mnist',\n",
        "        train = False,\n",
        "        download = True,\n",
        "        transform = transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                        \n",
        "    batch_size = 128,\n",
        "    shuffle = False,\n",
        "    num_workers = 0)\n",
        "\n",
        "    torchvision.datasets.MNIST:\n",
        "\n",
        "This function loads the MNIST dataset.\n",
        "'mnist': Specifies the directory where the dataset will be stored.\n",
        "\n",
        "    train = False:\n",
        "\n",
        "Indicates that we want to load the test dataset, not the training dataset.\n",
        "\n",
        "    download = True:\n",
        "\n",
        "Instructs the function to download the dataset if it's not already present in the specified directory.\n",
        "\n",
        "##transform = transform:\n",
        "Applies the previously defined transformation (transform) to the images as they are loaded.\n",
        "\n",
        "##torch.utils.data.DataLoader:\n",
        "This function creates a data loader that is used to iterate through the dataset in batches.\n",
        "\n",
        "##testset:\n",
        "The dataset to be loaded.\n",
        "\n",
        "###batch_size = 128:\n",
        "Specifies the number of images to be processed in each batch.\n",
        "\n",
        "###shuffle = False:\n",
        "Indicates that the data should not be shuffled.\n",
        "\n",
        "This is important for evaluation, as we want to maintain the original order of the data.\n",
        "\n",
        "##num_workers = 0:\n",
        "Sets the number of worker processes to be used for data loading.\n",
        "\n",
        "0 means that the data will be loaded in the main process.\n",
        "\n",
        "--------\n",
        "#In summary:\n",
        "\n",
        "This code snippet prepares the MNIST dataset for testing a PyTorch model by:\n",
        "\n",
        "Defining a transformation that converts images to PyTorch tensors and normalizes their pixel values.\n",
        "\n",
        "\n",
        "Loading the MNIST test dataset and applying the transformation to the images.\n",
        "\n",
        "Creating a data loader to iterate through the dataset in batches.\n",
        "\n",
        "This process ensures that the data is in the correct format and ready to be fed into the PyTorch model for evaluation."
      ],
      "metadata": {
        "id": "Av66_bO6kmqJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPjBF9lvHgOf"
      },
      "source": [
        "### **Creating our Model Defination Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHYzWFMX9S2a"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 12 * 12)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64r6fahLHqUE"
      },
      "source": [
        "# **2. Loading Out Model**\n",
        "\n",
        "I have uploaded the model to my Google Drive - https://drive.google.com/file/d/1yj01iUbYL8ZXHiYRE5Xd639tddSAkzKs/view?usp=sharing\n",
        "\n",
        "We use gdown in our terminal to download the model file that we trained in the last lesson.\n",
        "\n",
        "March 4th 2022 Update: File moved to S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQSj-eam-eVM",
        "outputId": "c94929f0-89c4-4649-afc3-df8ad4c26a4d"
      },
      "source": [
        "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/mnist_cnn_net.pth"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-01 22:48:39--  https://moderncomputervision.s3.eu-west-2.amazonaws.com/mnist_cnn_net.pth\n",
            "Resolving moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)... 52.95.150.66, 52.95.149.126, 52.95.149.178, ...\n",
            "Connecting to moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)|52.95.150.66|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-11-01 22:48:40 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13IzbMszH-_x"
      },
      "source": [
        "#### **NOTE**\n",
        "\n",
        "When Loading our model we need to create the model instance i.e. ```net = Net()``` and then since we trained it using our GPU in Colab, we move it to the GPU using ```net.to(device``` where device = 'cuda'.\n",
        "\n",
        "Then we can load our downloaded model's weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "sQMSMuS64C3Z",
        "outputId": "c9b6d942-2898-46e8-8ea2-afa605104f80"
      },
      "source": [
        "# Create an instance of the model\n",
        "net = Net()\n",
        "net.to(device)\n",
        "\n",
        "# Load weights from the specified path\n",
        "net.load_state_dict(torch.load('mnist_cnn_net.pth'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-bd6ca3d081df>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load('mnist_cnn_net.pth'))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mnist_cnn_net.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bd6ca3d081df>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load weights from the specified path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_cnn_net.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist_cnn_net.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppYoj4uPIj7g"
      },
      "source": [
        "Model Loaded successfully if ```All keys matched successfully``` is displayed.\n",
        "\n",
        "### **Now Let's calculate it's accuracy (done in the prevoiusly lesson so this is just a recap) on the Test Data**\n",
        "\n"
      ]
    },
    {
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "# Assuming you have a CUDA-enabled device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 12 * 12)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Create an instance of the model\n",
        "net = Net()\n",
        "net.to(device)\n",
        "\n",
        "# Download the weights file and specify the file path\n",
        "# Make sure this cell is executed before loading the weights\n",
        "weights_path = 'mnist_cnn_net.pth'\n",
        "if not os.path.exists(weights_path):\n",
        "    !wget -O {weights_path} https://moderncomputervision.s3.eu-west-2.amazonaws.com/mnist_cnn_net.pth\n",
        "\n",
        "\n",
        "# Load weights from the specified path\n",
        "# the file should be in the current working directory\n",
        "net.load_state_dict(torch.load(weights_path))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "xIIXJbCG6_LE",
        "outputId": "891bf3fc-0916-40fe-8f9e-4de5bcf5135f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "--2024-11-01 22:50:14--  https://moderncomputervision.s3.eu-west-2.amazonaws.com/mnist_cnn_net.pth\n",
            "Resolving moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)... 52.95.144.2, 52.95.142.110, 52.95.150.138, ...\n",
            "Connecting to moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)|52.95.144.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-11-01 22:50:14 ERROR 404: Not Found.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-88abb26fc1e6>:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(weights_path))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "Ran out of input",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-88abb26fc1e6>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Load weights from the specified path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# the file should be in the current working directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m         return _legacy_load(\n\u001b[0m\u001b[1;32m   1385\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         )\n\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaLl8YeiIjBO"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Move our data to GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the network on the 10000 test images: {accuracy:.3}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxIH5LgyIcM1"
      },
      "source": [
        "# **3. Displaying our Misclassified Images** ##\n",
        "\n",
        "Out of 10,000 images our model predicted 98.7% correct. This is good for a first attempt with such a simple model. (there are much better models).\n",
        "\n",
        "**A Good Practise!**\n",
        "\n",
        "It's a good habit when creating image classifiers to visually inspect the images being mis-classified.\n",
        "1. We can spot what types of images are challenging for our model\n",
        "2. We can spot any incorrectly labeled images\n",
        "3. If sometimes we can't correctly identify the class, seeing your CNN struggle hurts less :)\n",
        "\n",
        "**Reminder** on why we use ```net.eval()``` and ```torch.no_grad()```\n",
        "\n",
        "[Taken from Stackoverflow:](https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch)\n",
        "\n",
        "**model.eval()** is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, **Dropouts** Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation.\n",
        "\n",
        "So, while we don't use Dropouts or BatchNorm in our model, it's good practice to use it when doing inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbaEduiz-q85"
      },
      "source": [
        "# Set model to evaluation or inference mode\n",
        "net.eval()\n",
        "\n",
        "# We don't need gradients for validation, so wrap in\n",
        "# no_grad to save memory\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "\n",
        "        # Move our data to GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Get our outputs\n",
        "        outputs = net(images)\n",
        "\n",
        "        # use torch.argmax() to get the predictions, argmax is used for long_tensors\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # For test data in each batch we identify when predictions did not match the labe\n",
        "        # then we print out the actual ground truth\n",
        "        for i in range(data[0].shape[0]):\n",
        "            pred = predictions[i].item()\n",
        "            label = labels[i]\n",
        "            if(label != pred):\n",
        "                print(f'Actual Label: {label}, Predicted Label: {pred}')\n",
        "                img = np.reshape(images[i].cpu().numpy(),[28,28])\n",
        "                imgshow(\"\", np.uint8(img), size = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmyBsRZ7KWXu"
      },
      "source": [
        "# **4. Creating our Confusion Matrix**\n",
        "\n",
        "We use Sklean's Confusion Matrix tool to create it. All we need is:\n",
        "1. The true labels\n",
        "2. The predicted labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuh9HZ4RCa2Q"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# Initialize blank tensors to store our predictions and labels lists(tensors)\n",
        "pred_list = torch.zeros(0, dtype=torch.long, device='cpu')\n",
        "label_list = torch.zeros(0, dtype=torch.long, device='cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(testloader):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Append batch prediction results\n",
        "        pred_list = torch.cat([pred_list, preds.view(-1).cpu()])\n",
        "        label_list = torch.cat([label_list, classes.view(-1).cpu()])\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat = confusion_matrix(label_list.numpy(), pred_list.numpy())\n",
        "print(conf_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7xP1q5LNGmr"
      },
      "source": [
        "#### **Interpreting the Confusion Matrix**\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/CleanShot%202020-11-30%20at%2010.46.45.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_htE-x-hOWqx"
      },
      "source": [
        "### **Creating a more presentable plot**\n",
        "\n",
        "We'll reuse this nicely done function from the sklearn documentation on plotting a confusion matrix using color gradients and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_DFGhuRNFm0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpKTzF-iNkWx"
      },
      "source": [
        "target_names = list(range(0,10))\n",
        "plot_confusion_matrix(conf_mat, target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7t3zgNwO6Gu"
      },
      "source": [
        "## **Let's look at our per-class accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBOgE7FFDG8c"
      },
      "source": [
        "# Per-class accuracy\n",
        "class_accuracy = 100 * conf_mat.diagonal() / conf_mat.sum(1)\n",
        "\n",
        "for (i,ca) in enumerate(class_accuracy):\n",
        "    print(f'Accuracy for {i} : {ca:.3f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVyqrg7FNNfe"
      },
      "source": [
        "# **5. Now let's look at the Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2szDhgWF0OH"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(label_list.numpy(), pred_list.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD6AUJ04Sane"
      },
      "source": [
        "### **5.1 Support is the total sum of that class in the dataset**\n",
        "\n",
        "### **5.2 Review of Recall**\n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/CleanShot%202020-11-30%20at%2011.11.12.png)\n",
        "\n",
        "### **5.3 Review of Precision**\n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/CleanShot%202020-11-30%20at%2011.11.22.png)\n",
        "\n",
        "### **5.4 High recall (or sensitivity) with low precision.**\n",
        "This tells us that most of the positive examples are correctly recognized (low False Negatives) but there are a lot of false positives i.e. other classes being predicted as our class in question.\n",
        "\n",
        "### **5.5 Low recall (or sensitivity) with high precision.**\n",
        "\n",
        "Our classifier is missing a lot of positive examples (high FN) but those we predaict as positive are indeed positive (low False Positives)\n"
      ]
    }
  ]
}